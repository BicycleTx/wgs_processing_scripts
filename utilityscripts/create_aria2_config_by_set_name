#!/usr/bin/env bash

source message_functions || exit 1

command -v hmf_api_get > /dev/null || die "Dependency hmf_api_get not found"
command -v jq > /dev/null || die "Dependency jq not found"

set_name=$1

if [[ -z "${set_name}" ]]; then
    echo "-----"
    echo " Usage: $(basename "$0") \${set-name}"
    echo " Exmpl: $(basename "$0") 210319_HMFregCORE_FR30729774_FR23588534_COREDB010063"
    echo "-----"
    exit 1
fi

credentials_dir="/data/common/dbs/gcp_credentials"
key_file="${credentials_dir}/hmf-ops"
project="hmf-database"
account="hmf-ops"
service_account="${account}@${project}.iam.gserviceaccount.com"

info "Start of $(basename $0)"
info "Switching to service account ${service_account}"
msg=$(gcloud config set account "${service_account}" 2>&1)
info "${msg}"

function main() {
    ## make sure only one run exists for the set
    runs_json=$(hmf_api_get "runs?set_name=${set_name}" | jq -c '[.[] | select(.ini != "Rerun.ini")]') || die "Unable to get run (${set_name})"
    runs_count=$(jq '. | length' <<< "${runs_json}")
    if [[ "${runs_count}" -ne 1 ]]; then
        die "Only sets supported with exactly one run (found ${runs_count}). Exiting"
    fi
    run_json=$(echo "${runs_json}" | jq -r '.[-1]')

    ## setup all variables
    cluster=$(jq -r '.cluster' <<< "${run_json}")
    bucket=$(jq -r '.bucket' <<< "${run_json}")
    entity_id=$(jq -r '.set.entity_id' <<< "${run_json}")
    status=$(jq -r '.status' <<< "${run_json}")
    name=$(jq -r '.set.name' <<< "${run_json}")
    pip=$(jq -r '.version' <<< "${run_json}")
    ini=$(jq -r '.ini' <<< "${run_json}")
    ref=$(jq -r '.set.ref_sample' <<< "${run_json}")
    tum=$(jq -r '.set.tumor_sample' <<< "${run_json}")
    oid=$(jq -r '.id' <<< "${run_json}") # object id

    out_jsn_all="${name}_runfiles.json"
    out_md5_all="${name}_runfiles.md5"
    out_aria="${name}.aria.txt"
    out_md5="${name}.md5"
    out_url_int="${name}_internal_urls.txt"
    out_url_ext="${name}_external_urls.txt"

    ## create re-usable list of all output files
    output_files=("${out_jsn_all}" "${out_md5_all}" "${out_aria}" "${out_md5}" "${out_url_int}" "${out_url_ext}")

    info "Details:"
    info "  RunName: ${name} (oid=${oid})"
    info "  RunStat: ${status}"
    info "  IniName: ${ini}"
    info "  TumName: ${tum}"
    info "  RefName: ${ref}"
    info "  PipeVsn: ${pip}"
    info "  Cluster: ${cluster}"
    info "  Bucket:  ${bucket}"
    info "  Entity:  ${entity_id}"

    ## cleanup existing files
    for output_file in "${output_files[@]}"; do
        if [[ -f "${output_file}" ]]; then
            info "Deleting existing file (${output_file})" && rm "${output_file}";
        fi
    done

    ## get the file objects for one run by id
    info "Collecting required information"
    files_json=$(hmf_api_get "files?run_id=${oid}")

    ## create run type agnostic info files
    create_json_file "${files_json}" "${out_jsn_all}"
    create_md5sums_file "${files_json}" "${out_md5_all}"

    ## create the run type specific subset files
    if [[ "${ini}" == "FastQ.ini" || "${ini}" == "Rna.ini" ]]; then
        create_run_specific_files "${files_json}" \
        "${out_md5}" "${out_aria}" "${out_url_int}" "${out_url_ext}" "${name}" "${bucket}" "${ref}" "${tum}" "${pip}" "Fastq"
    elif [[ "${ini}" == "Somatic.ini" ]]; then
        create_run_specific_files "${files_json}" \
        "${out_md5}" "${out_aria}" "${out_url_int}" "${out_url_ext}" "${name}" "${bucket}" "${ref}" "${tum}" "${pip}" "Somatic"
    elif [[ "${ini}" == "SingleSample.ini" ]]; then
        create_run_specific_files "${files_json}" \
        "${out_md5}" "${out_aria}" "${out_url_int}" "${out_url_ext}" "${name}" "${bucket}" "${ref}" "${tum}" "${pip}" "SingleSample"
    elif [[ "${ini}" == "ShallowSeq.ini" ]]; then
        create_run_specific_files "${files_json}" \
        "${out_md5}" "${out_aria}" "${out_url_int}" "${out_url_ext}" "${name}" "${bucket}" "${ref}" "${tum}" "${pip}" "ShallowSeq"
    else
        die "Unknown ini (${ini})"
    fi

    ## output check
    info "Output files:"
    for output_file in "${output_files[@]}"; do
        if [[ ! -f "${output_file}" ]]; then
            die "Output file ${output_file} not found!"
        else
            line_count=$(wc -l "${output_file}" | cut -d" " -f1)
            info "  ${output_file} (${line_count} lines)"
        fi
    done

    info "Finished with $(basename "$0")"
}

create_run_specific_files () {
    local json=$1 && shift
    local out_md5=$1 && shift
    local out_aria=$1 && shift
    local out_url_int=$1 && shift
    local out_url_ext=$1 && shift
    local name=$1 && shift
    local bucket=$1 && shift
    local ref=$1 && shift
    local tum=$1 && shift
    local pip=$1 && shift
    local run_type=$1 && shift
    local minor=""

    if [[ "${run_type}" == "Fastq" ]]; then
        pip="Fastq"
    elif [[ "${pip}" =~ ^5\.[0-9]+$ ]]; then
        minor=$(echo "${pip}" | cut -d"." -f2)
    else
        die "Pipeline version string has unknown format (${pip})"
        exit 1
    fi
 
    ## from pipeline v5.10 onwards CRAM replaced BAM
    if [[ "${pip}" == "Fastq" ]]; then
        ## for fastq type runs no pipeline output is available
        ref_bam="undef"
        ref_bam_bai="undef"
        tumor_bam="undef"
        tumor_bam_bai="undef"
    elif [[ "${minor}" -lt 10 ]]; then
        ref_bam="${ref}/aligner/${ref}.bam"
        ref_bam_bai="${ref}/aligner/${ref}.bam.bai"
        tumor_bam="${tum}/aligner/${tum}.bam"
        tumor_bam_bai="${tum}/aligner/${tum}.bam.bai"
    else
        ref_bam="${ref}/cram/${ref}.cram"
        ref_bam_bai="${ref}/cram/${ref}.cram.crai"
        tumor_bam="${tum}/cram/${tum}.cram"
        tumor_bam_bai="${tum}/cram/${tum}.cram.crai"
    fi

    mapfile -t all_file_paths < <( echo "${json}" | jq -r '.[].filepath' )

    local single_files=(
        "${ref_bam}"
        "${ref_bam_bai}"
        "${ref}/germline_caller/${ref}.germline.vcf.gz"
        "${ref}/germline_caller/${ref}.germline.vcf.gz.tbi"
    )

    local shallow_files=(
        "purple/purple.version"
        "purple/${tum}.purple.cnv.somatic.tsv"
        "purple/${tum}.purple.cnv.gene.tsv"
        "purple/${tum}.purple.purity.tsv"
        "purple/${tum}.purple.purity.range.tsv"
        "purple/${tum}.purple.qc"
        "purple/${tum}.purple.sv.vcf.gz"
        "purple/${tum}.purple.sv.vcf.gz.tbi"
        "purple/${tum}.purple.somatic.vcf.gz"
        "purple/${tum}.purple.somatic.vcf.gz.tbi"
        "purple/${tum}.purple.cnv.germline.tsv"
        "purple/plot/${tum}.circos.png"
        "${ref_bam}"
        "${ref_bam_bai}"
        "${tumor_bam}"
        "${tumor_bam_bai}"
    )

    driver_cat_paths=(
        "purple/${tum}.driver.catalog.germline.tsv"
        "purple/${tum}.driver.catalog.somatic.tsv"
    )

    # No germline driver catalog if older than 5.19
    if [[ "${minor}" -lt 19 ]]; then
        driver_cat_paths=("purple/${tum}.driver.catalog.tsv")
    fi

    local somatic_files=(
        "purple/purple.version"
        "purple/${tum}.purple.cnv.somatic.tsv"
        "purple/${tum}.purple.cnv.gene.tsv"
        "purple/${tum}.purple.purity.tsv"
        "purple/${tum}.purple.purity.range.tsv"
        "purple/${tum}.purple.qc"
        "purple/${tum}.purple.sv.vcf.gz"
        "purple/${tum}.purple.sv.vcf.gz.tbi"
        "purple/${tum}.purple.somatic.vcf.gz"
        "purple/${tum}.purple.somatic.vcf.gz.tbi"
        "purple/${tum}.purple.cnv.germline.tsv"
        "purple/plot/${tum}.circos.png"
        "${ref_bam}"
        "${ref_bam_bai}"
        "${tumor_bam}"
        "${tumor_bam_bai}"
        "${ref}/germline_caller/${ref}.germline.vcf.gz"
        "${ref}/germline_caller/${ref}.germline.vcf.gz.tbi"
        "${driver_cat_paths[@]}"
    )

    ## select file collection
    if [[ ${run_type} == "Somatic" ]]; then
        file_selection=("${somatic_files[@]}")
    elif [[ ${run_type} == "ShallowSeq" ]]; then
        file_selection=("${shallow_files[@]}")
    elif [[ ${run_type} == "SingleSample" ]]; then
        file_selection=("${single_files[@]}")
    elif [[ ${run_type} == "Fastq" ]]; then
        file_selection=("${all_file_paths[@]}")
    else
        die "Unknown run type (${run_type})"
    fi

    ## output aria2 config file
    info "Creating ${out_aria} (and selection tmp files)"
    for file_path in "${file_selection[@]}"; do
        file_name=$(basename "${file_path}")
        file_in_bucket="${bucket}/${name}/${file_path}"
        gs_url="gs://${file_in_bucket}"

        ## adjust for Fastq files (those already are complete internal gs path)
        if [[ ${run_type} == "Fastq" ]]; then
            file_in_bucket=${file_path/gs\:\/\//}
            gs_url="${file_path}"
        fi

        ## get file specific md5 hash
        md5sum=$(echo "${json}" | jq -r --arg url "${gs_url}" '.[] | select(.filepath == $url) | .hash')
        [[ -n "${md5sum}" ]] || die "Missing md5 hash for file at url (${gs_url})"
        printf '%s\t%s\n' "${md5sum}" "${file_name}" >> "${out_md5}"

        ## sanity check on existence of file in bucket
        gsutil -u "${project}" -q stat "${gs_url}"
        if [[ $? -eq 1 ]]; then
            msg=$(gsutil -u "${project}" ls "${gs_url}" 2>&1)
            warn "Cannot access file so skipping (${gs_url})"
            warn "ErrCode=${?} ErrMsg=\"${msg}\""
            continue
        fi

        ## get pre-signed URL (prints a header and multiple fields)
        url=$(gsutil signurl -r europe-west4 -b "${project}" -d 7d "${key_file}" "${gs_url}" | grep -v 'Signed URL' | cut -f4)

        ## final sanity checks
        [[ "${url}" =~ ^https ]] || die "Pre-signed URL does not start with https (${url})"
        [[ -n "${name}" ]] || die "Dir not defined for aria config (${name})"
        [[ -n "${md5sum}" ]] || die "Md5sum not defined for aria config (${name})"

        ## print aria2 config for one file (for options see https://aria2.github.io/manual/en/html/aria2c.html#input-file)
        {
          echo "${url}"
          echo "  dir=${name}"
          echo "  checksum=md5=${md5sum}"
          echo ""
        } >> "${out_aria}"

        ## write urls to separate log files
        echo "${gs_url}" >> "${out_url_int}"
        echo "${url}" >> "${out_url_ext}"
    done
}

create_json_file () {
    local json_text=$1 && shift
    local out_file=$1 && shift
    info "Creating ${out_file}"
    echo "${json_text}" | jq '.' > "${out_file}"
}

create_md5sums_file () {
    local json_text=$1 && shift
    local out_file=$1 && shift
    info "Creating ${out_file}"
    echo "${json_text}" | jq -r '.[] | select(.directory == "") | .hash + "  " + .filename' > "${out_file}"
    echo "${json_text}" | jq -r '.[] | select(.directory != "") | .hash + "  " + .directory + "/" + .filename' >> "${out_file}"
}

main

