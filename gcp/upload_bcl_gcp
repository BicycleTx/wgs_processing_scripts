#!/usr/bin/env bash

set -e

flowcells_dir="/data1/illumina_data"
bucket="bcl-input-prod-1"
gcp_cred="/data/dbs/gcp_credentials/bcl-input-prod"

function exit_handler() {
    if [[ $? -ne 0 ]]; then
        gcloud logging write on_premises "ERROR while uploading flowcells from $(hostname)" --severity=ERROR --project=hmf-pipeline-prod-e45b00f2
    fi
}

function rsync_bcls() {
    gcloud auth activate-service-account --key-file "${gcp_cred}"
    gsutil -m -o GSUtil:parallel_process_count=7 -o GSUtil:parallel_thread_count=1 -o GSUtil:parallel_composite_upload_threshold=350M \
        rsync -r -x ".*Fastq.*|.*Logs.*|.*Images.*|.*PeriodicSaveRates.*|.*fastq\.gz|.*BaseCalls/[^L].*|.*\.tmp\..*|RTAComplete.txt" "$1" "$2"
}

trap exit_handler EXIT

while true; do
    find "${flowcells_dir}" -mindepth 1 -maxdepth 1 -type d -not -name "TestRuns" -not -name "MyRun" | while read -r flowcell_path; do
        upload_complete="${flowcell_path}/GCPUploadComplete.txt"
        if [[ ! -f $upload_complete && -d $flowcell_path ]]; then
            echo "Starting BCL upload of ${flowcell_path}"
            gs_path="gs://${bucket}/$(basename "$flowcell_path")"
            rsync_bcls "$flowcell_path" "$gs_path"
            local_data_complete="${flowcell_path}/RTAComplete.txt"
            if ( find ${flowcell_path} -maxdepth 1 -name "RTAComplete.txt" -mmin +20 | grep . ); then
                echo "Upload from sequencer is now complete, doing a final sync of ${flowcell_path}"
                date --date="@$(stat -c '%Y' "$local_data_complete")" "+%Y-%m-%d %H:%M:%S %z" > "${flowcell_path}/RTAComplete.timestamp"
                rsync_bcls "$flowcell_path" "$gs_path"
                gsutil cp "$local_data_complete" "$gs_path"
                echo "Upload of ${flowcell_path} succeeded"
                date "+%Y-%m-%d %H:%M:%S %z" > "$upload_complete"
            fi
        fi
    done
done

